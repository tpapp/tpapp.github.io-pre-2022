<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bayesian on Tamás K. Papp&#39;s website</title>
  <link href="https://tpapp.github.io/index.xml" rel="self"/>
  <link href="https://tpapp.github.io/tags/bayesian/"/>
  <updated>2017-06-12T16:25:57+02:00</updated>
  
  <id>https://tpapp.github.io/tags/bayesian/</id>
  <author>
    <name>Tamás K. Papp</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  
  <entry>
    <title type="html">Sampling variation in effective sample size estimates (MCMC)</title>
    <link href="https://tpapp.github.io/post/ess-sampling/"/>
    <id>https://tpapp.github.io/post/ess-sampling/</id>
    <published>2017-06-12T16:25:57+02:00</published>
    <updated>2017-06-12T16:25:57+02:00</updated>
    
    <content type="html" xml:base="https://tpapp.github.io/post/ess-sampling/">&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;MCMC samples, used in Bayesian statistics, are not independent --- in fact, unless one uses specialized methods or &lt;a href=&#34;https://arxiv.org/abs/1701.02434&#34;&gt;modern HMC&lt;/a&gt;, posterior draws are usually at highly autocorrelated. For independent draws,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\text{variance of simulation mean} \propto \frac1N
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; is the sample size, but for correlated draws, one has to scale the sample size with a factor&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\tau = \frac{1}{1+2\sum_{k=1}^\infty \rho_k}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(\rho_k\)&lt;/span&gt; is the lag-&lt;span  class=&#34;math&#34;&gt;\(k\)&lt;/span&gt; autocorrelation. &lt;span  class=&#34;math&#34;&gt;\(\tau N\)&lt;/span&gt; is the &lt;em&gt;effective sample size&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Usually, &lt;span  class=&#34;math&#34;&gt;\(\rho_k\)&lt;/span&gt; is estimated from the data using the variogram&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
V_k = \frac{1}{N-k} \sum_{i=1}^{N-k} x_i x_{i+k}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;from which we obtain&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\rho_k = 1-\frac{V_k}{2\text{var}(x)}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where an estimator for the variance is also used. Then, to avoid using noisy estimates, we only add up to the last &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt; where&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\rho_{K} + \rho_{K+1} \ge 0
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I will call &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt; the &lt;em&gt;last lag&lt;/em&gt;. &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; does something slightly different, using FFT for autocorrelations, and cutting off at the first negative &lt;span  class=&#34;math&#34;&gt;\(\rho_K\)&lt;/span&gt;, but for HMC this does not make a whole lot of difference.&lt;/p&gt;

&lt;h2 id=&#34;the-sampling-variation&#34;&gt;The sampling variation&lt;/h2&gt;

&lt;p&gt;I was coding up the above calculation, and needed some unit tests. Surprisignly, I could not find anything on the sampling variation of &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, so I wrote some simulations in Julia (&lt;a href=&#34;../ess-sampling.jl&#34;&gt;source code for everything&lt;/a&gt;). I did the following simulation exercise:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;for a given autocorrelation coefficient &lt;span  class=&#34;math&#34;&gt;\(\phi\)&lt;/span&gt;, simulate &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; draws from the AR(1) process
&lt;span  class=&#34;math&#34;&gt;\(
x_t = \phi x_{t-1} + \sigma \epsilon_t
\qquad
\epsilon_t \sim \text{Normal}(0,1), \text{IID}
\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;calculate &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;repeat 1000 times and plot the results.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I use &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;, as these would be typical sample sizes, first for a fairly efficient algorithm, then for a more stubborn but still manageable posterior.&lt;/p&gt;

&lt;h2 id=&#34;iid-samples&#34;&gt;IID samples&lt;/h2&gt;

&lt;p&gt;Let &lt;span  class=&#34;math&#34;&gt;\(\phi=0\)&lt;/span&gt;, then we expect &lt;span  class=&#34;math&#34;&gt;\(\tau=1\)&lt;/span&gt; (red line in histogram, coefficient of variation on top).&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0\)&lt;/span&gt; (IID), &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi0N1000.svg&#34; alt=&#34;Results with $$\phi=0$$ (IID), $$N=1000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0\)&lt;/span&gt; (IID), &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi0N10000.svg&#34; alt=&#34;Results with $$\phi=0$$ (IID), $$N=10000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;With &lt;span  class=&#34;math&#34;&gt;\(1000\)&lt;/span&gt; samples, there is a lot of variation in ESS: 800 could show up very easily in practice. &lt;span  class=&#34;math&#34;&gt;\(600\)&lt;/span&gt; is not improbable either. Using up to &lt;span  class=&#34;math&#34;&gt;\(10\)&lt;/span&gt; lags is not uncommon. For &lt;span  class=&#34;math&#34;&gt;\(10000\)&lt;/span&gt; samples, the precision is improved considerably, we commonly use &lt;span  class=&#34;math&#34;&gt;\(2\)&lt;/span&gt; or &lt;span  class=&#34;math&#34;&gt;\(4\)&lt;/span&gt; lags. For both sample sizes, notice the high correlation between the last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, and &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;: given the method above, using more lags increases &lt;span  class=&#34;math&#34;&gt;\(\tau^{-1}\)&lt;/span&gt;, so this is to be expected.&lt;/p&gt;

&lt;h2 id=&#34;ar1-samples-with-rho05&#34;&gt;AR(1) samples with &lt;span  class=&#34;math&#34;&gt;\(\rho=0.5\)&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;This is a more autocorrelated process, here theory tells us that &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;=1/3.&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.5\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi05N1000.svg&#34; alt=&#34;Results with $$\phi=0.5$$, $$N=1000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.5\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi05N10000.svg&#34; alt=&#34;Results with $$\phi=0.5$$, $$N=10000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Notice that &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt; is now more dispersed, compared to the IID case. Even with 10000 samples, the coefficient of variation is 6%, with 1000 it is around 1/6. In practice, expect effective sample sizes all over the place.&lt;/p&gt;

&lt;h2 id=&#34;ar1-samples-with-rho08&#34;&gt;AR(1) samples with &lt;span  class=&#34;math&#34;&gt;\(\rho=0.8\)&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;This is an even more autocorrelated process, here theory tells us that &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;=1/9.&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.8\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi08N1000.svg&#34; alt=&#34;Results with $$\phi=0.8$$, $$N=1000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.8\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi08N10000.svg&#34; alt=&#34;Results with $$\phi=0.8$$, $$N=10000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;There is now so much variation that in order to get an estimate for ESS that we can use for comparing various MCMC implementations, we need to run much more than &lt;span  class=&#34;math&#34;&gt;\(1000\)&lt;/span&gt; samples.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For unit testing ESS calculations, I will need to use 10000 samples, with &lt;span  class=&#34;math&#34;&gt;\(\pm10\)&lt;/span&gt; or similar error bands.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As a rule of thumb, I would ignore less than 1.5x variation in ESS for 1000 samples, or run longer chains: it may be just random noise.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;bibliography&#34;&gt;Bibliography&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Gelman, Andrew, et al. 2013. Bayesian data analysis. 3rd edition. Chapman &amp;amp; Hall/CRC.&lt;/li&gt;
&lt;li&gt;Stan Development Team. 2016. Stan Modeling Language Users Guide and Reference Manual, Version 2.15.0. &lt;a href=&#34;http://mc-stan.org&#34;&gt;http://mc-stan.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
</feed>
