<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MCMConTamás K. Papp&#39;s website</title>
  <link href="http://tpapp.github.io/index.xml" rel="self"/>
  <link href="http://tpapp.github.io/tags/mcmc/"/>
  <updated>2017-06-12T16:25:57+02:00</updated>
  
  <id>http://tpapp.github.io/tags/mcmc/</id>
  <author>
    <name>Tamás K. Papp</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  
  <entry>
    <title type="html">Sampling variation in effective sample size estimates (MCMC)</title>
    <link href="http://tpapp.github.io/post/ess-sampling/ess-sampling/"/>
    <id>http://tpapp.github.io/post/ess-sampling/ess-sampling/</id>
    <published>2017-06-12T16:25:57+02:00</published>
    <updated>2017-06-12T16:25:57+02:00</updated>
    
    <content type="html" xml:base="http://tpapp.github.io/post/ess-sampling/ess-sampling/">

&amp;lt;h2 id=&amp;#34;introduction&amp;#34;&amp;gt;Introduction&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;MCMC samples, used in Bayesian statistics, are not independent &amp;amp;mdash; in fact, unless one uses specialized methods or &amp;lt;a href=&amp;#34;https://arxiv.org/abs/1701.02434&amp;#34;&amp;gt;modern HMC&amp;lt;/a&amp;gt;, posterior draws are usually at highly autocorrelated. For independent draws,
[
\text{variance of simulation mean} \propto \frac1N
]
where $N$ is the sample size, but for correlated draws, one has to scale the sample size with a factor
[
\tau = \frac{1}{1&#43;2\sum_{k=1}^\infty \rho_k}
]
where $\rho_k$ is the lag-$k$ autocorrelation. $\tau N$ is the &amp;lt;em&amp;gt;effective sample size&amp;lt;/em&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;Usually, $\rho_k$ is estimated from the data using the variogram
[
V_k = \frac{1}{N-k} \sum_{i=1}^{N-k} x_i x_{i&#43;k}
]
from which we obtain
[
\rho_k = 1-\frac{V_k}{2\text{var}(x)}
]
where an estimator for the variance is also used. Then, to avoid using noisy estimates, we only add up to the last $K$ where
[
\rho_{K} &#43; \rho_{K&#43;1} \ge 0
]
I will call $K$ the &amp;lt;em&amp;gt;last lag&amp;lt;/em&amp;gt;. &amp;lt;a href=&amp;#34;http://mc-stan.org/&amp;#34;&amp;gt;Stan&amp;lt;/a&amp;gt; does something slightly different, using FFT for autocorrelations, and cutting off at the first negative $\rho_K$, but for HMC this does not make a whole lot of difference.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;the-sampling-variation&amp;#34;&amp;gt;The sampling variation&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;I was coding up the above calculation, and needed some unit tests. Surprisignly, I could not find anything on the sampling variation of $\tau$, so I wrote some simulations in Julia (&amp;lt;a href=&amp;#34;../ess-sampling.jl&amp;#34;&amp;gt;source code for everything&amp;lt;/a&amp;gt;). I did the following simulation exercise:&amp;lt;/p&amp;gt;

&amp;lt;ol&amp;gt;
&amp;lt;li&amp;gt;for a given autocorrelation coefficient $\phi$, simulate $N$ draws from the AR(1) process
[
x_t = \phi x_{t-1} &#43; \sigma \epsilon_t
\qquad
\epsilon_t \sim \text{Normal}(0,1), \text{IID}
]&amp;lt;/li&amp;gt;
&amp;lt;li&amp;gt;calculate $\tau$ and $K$,&amp;lt;/li&amp;gt;
&amp;lt;li&amp;gt;repeat 1000 times and plot the results.&amp;lt;/li&amp;gt;
&amp;lt;/ol&amp;gt;

&amp;lt;p&amp;gt;I use $N=1000$ and $N=10000$, as these would be typical sample sizes, first for a fairly efficient algorithm, then for a more stubborn but still manageable posterior.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;iid-samples&amp;#34;&amp;gt;IID samples&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Let $\phi=0$, then we expect $\tau=1$ (red line in histogram, coefficient of variation on top).&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
&amp;lt;figure &amp;gt;
    
        &amp;lt;img src=&amp;#34;../ess-phi0N1000.svg&amp;#34; /&amp;gt;
    
    
    &amp;lt;figcaption&amp;gt;
        &amp;lt;h4&amp;gt;Results with $\phi=0$ (IID), $N=1000$. (a) $\tau$, (b) last lag $K$, (c) scatterplot.&amp;lt;/h4&amp;gt;
        
    &amp;lt;/figcaption&amp;gt;
    
&amp;lt;/figure&amp;gt;


&amp;lt;figure &amp;gt;
    
        &amp;lt;img src=&amp;#34;../ess-phi0N10000.svg&amp;#34; /&amp;gt;
    
    
    &amp;lt;figcaption&amp;gt;
        &amp;lt;h4&amp;gt;Results with $\phi=0$ (IID), $N=10000$. (a) $\tau$, (b) last lag $K$, (c) scatterplot.&amp;lt;/h4&amp;gt;
        
    &amp;lt;/figcaption&amp;gt;
    
&amp;lt;/figure&amp;gt;
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;With $1000$ samples, there is a lot of variation in ESS: 800 could show up very easily in practice. $600$ is not improbable either. Using up to $10$ lags is not uncommon.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For $10000$ samples, the precision is improved considerably, we commonly use $2$ or $4$ lags.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For both sample sizes, notice the high correlation between the last lag $K$, and $\tau$: given the method above, using more lags increases $\tau^{-1}$, so this is to be expected.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;ar-1-samples-with-rho-0-5&amp;#34;&amp;gt;AR(1) samples with $\rho=0.5$&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;This is a more autocorrelated process, here theory tells us that $\tau$=&amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;&amp;amp;frasl;&amp;lt;sub&amp;gt;3&amp;lt;/sub&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
&amp;lt;figure &amp;gt;
    
        &amp;lt;img src=&amp;#34;../ess-phi05N1000.svg&amp;#34; /&amp;gt;
    
    
    &amp;lt;figcaption&amp;gt;
        &amp;lt;h4&amp;gt;Results with $\phi=0.5$, $N=1000$. (a) $\tau$, (b) last lag $K$, (c) scatterplot.&amp;lt;/h4&amp;gt;
        
    &amp;lt;/figcaption&amp;gt;
    
&amp;lt;/figure&amp;gt;


&amp;lt;figure &amp;gt;
    
        &amp;lt;img src=&amp;#34;../ess-phi05N10000.svg&amp;#34; /&amp;gt;
    
    
    &amp;lt;figcaption&amp;gt;
        &amp;lt;h4&amp;gt;Results with $\phi=0.5$, $N=10000$. (a) $\tau$, (b) last lag $K$, (c) scatterplot.&amp;lt;/h4&amp;gt;
        
    &amp;lt;/figcaption&amp;gt;
    
&amp;lt;/figure&amp;gt;
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;Notice that $\tau$ is now more dispersed, compared to the IID case. Even with 10000 samples, the coefficient of variation is 6%, with 1000 it is around &amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;&amp;amp;frasl;&amp;lt;sub&amp;gt;6&amp;lt;/sub&amp;gt;. In practice, expect effective sample sizes all over the place.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;ar-1-samples-with-rho-0-8&amp;#34;&amp;gt;AR(1) samples with $\rho=0.8$&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;This is an even more autocorrelated process, here theory tells us that $\tau$=&amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;&amp;amp;frasl;&amp;lt;sub&amp;gt;9&amp;lt;/sub&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
&amp;lt;figure &amp;gt;
    
        &amp;lt;img src=&amp;#34;../ess-phi08N1000.svg&amp;#34; /&amp;gt;
    
    
    &amp;lt;figcaption&amp;gt;
        &amp;lt;h4&amp;gt;Results with $\phi=0.8$, $N=1000$. (a) $\tau$, (b) last lag $K$, (c) scatterplot.&amp;lt;/h4&amp;gt;
        
    &amp;lt;/figcaption&amp;gt;
    
&amp;lt;/figure&amp;gt;


&amp;lt;figure &amp;gt;
    
        &amp;lt;img src=&amp;#34;../ess-phi08N10000.svg&amp;#34; /&amp;gt;
    
    
    &amp;lt;figcaption&amp;gt;
        &amp;lt;h4&amp;gt;Results with $\phi=0.8$, $N=10000$. (a) $\tau$, (b) last lag $K$, (c) scatterplot.&amp;lt;/h4&amp;gt;
        
    &amp;lt;/figcaption&amp;gt;
    
&amp;lt;/figure&amp;gt;
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;There is now so much variation that in order to get an estimate for ESS that we can use for comparing various MCMC implementations, we need to run much more than $1000$ samples.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;conclusion&amp;#34;&amp;gt;Conclusion&amp;lt;/h2&amp;gt;

&amp;lt;ol&amp;gt;
&amp;lt;li&amp;gt;&amp;lt;p&amp;gt;For unit testing ESS calculations, I will need to use 10000 samples, with $\pm10$ or similar error bands.&amp;lt;/p&amp;gt;&amp;lt;/li&amp;gt;

&amp;lt;li&amp;gt;&amp;lt;p&amp;gt;As a rule of thumb, I would ignore less than 1.5x variation in ESS for 1000 samples, or run longer chains: it may be just random noise.&amp;lt;/p&amp;gt;&amp;lt;/li&amp;gt;
&amp;lt;/ol&amp;gt;

&amp;lt;h2 id=&amp;#34;bibliography&amp;#34;&amp;gt;Bibliography&amp;lt;/h2&amp;gt;

&amp;lt;ul&amp;gt;
&amp;lt;li&amp;gt;Gelman, Andrew, et al. 2013. Bayesian data analysis. 3rd edition. Chapman &amp;amp;amp; Hall/CRC.&amp;lt;/li&amp;gt;
&amp;lt;li&amp;gt;Stan Development Team. 2016. Stan Modeling Language Users Guide and Reference Manual, Version 2.15.0. &amp;lt;a href=&amp;#34;http://mc-stan.org&amp;#34;&amp;gt;http://mc-stan.org&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Getting a nice &#43;= in LaTeX math</title>
    <link href="http://tpapp.github.io/post/latex-math-increment/"/>
    <id>http://tpapp.github.io/post/latex-math-increment/</id>
    <published>2017-05-24T12:59:53+02:00</published>
    <updated>2017-05-24T12:59:53+02:00</updated>
    
    <content type="html" xml:base="http://tpapp.github.io/post/latex-math-increment/">&amp;lt;p&amp;gt;I am working on an appendix for a paper that uses MCMC, and I decided to document some &amp;lt;a href=&amp;#34;http://tpapp.github.io/post/jacobian-chain/&amp;#34;&amp;gt;change of varible calculations&amp;lt;/a&amp;gt; in the interest of reproducibility (they are quite complex, because of multivariate determinants). But how can I typeset them nicely in $\LaTeX$?&amp;lt;/p&amp;gt;
&amp;lt;div class=&amp;#34;highlight&amp;#34;&amp;gt;&amp;lt;pre class=&amp;#34;chroma&amp;#34;&amp;gt;&amp;lt;code class=&amp;#34;language-latex&amp;#34; data-lang=&amp;#34;latex&amp;#34;&amp;gt;\mathtt{target} &#43;= J_f&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;p&amp;gt;gives
$$
\mathtt{target} &#43;= J_f
$$
which is to be expected, as &amp;lt;code&amp;gt;&#43;&amp;lt;/code&amp;gt; is a binary operator and &amp;lt;code&amp;gt;=&amp;lt;/code&amp;gt; is a relation, so $\LaTeX$ is not expecting them to show up this way.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;We can remedy this as&amp;lt;/p&amp;gt;
&amp;lt;div class=&amp;#34;highlight&amp;#34;&amp;gt;&amp;lt;pre class=&amp;#34;chroma&amp;#34;&amp;gt;&amp;lt;code class=&amp;#34;language-latex&amp;#34; data-lang=&amp;#34;latex&amp;#34;&amp;gt;\mathtt{target} \mathrel{&#43;}= J_f&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;p&amp;gt;which shows up as
$$
\mathtt{target} \mathrel{&#43;}= J_f
$$
which is an improvement, but is still not visually appealing.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;Making the &amp;lt;code&amp;gt;&#43;&amp;lt;/code&amp;gt; a bit smaller with&amp;lt;/p&amp;gt;
&amp;lt;div class=&amp;#34;highlight&amp;#34;&amp;gt;&amp;lt;pre class=&amp;#34;chroma&amp;#34;&amp;gt;&amp;lt;code class=&amp;#34;language-latex&amp;#34; data-lang=&amp;#34;latex&amp;#34;&amp;gt;\mathrel{\raisebox{0.19ex}{$\scriptstyle&#43;$}}=}&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;p&amp;gt;yields
$$
\mathtt{target} \mathrel{\raise{0.19ex}{\scriptstyle&#43;}} = J_f
$$
which looks OK enough to preclude further tweaking. Note that &amp;lt;a href=&amp;#34;http://www.mathjax.org/&amp;#34;&amp;gt;MathJax&amp;lt;/a&amp;gt; does not support &amp;lt;code&amp;gt;\raisebox&amp;lt;/code&amp;gt;, but you can use&amp;lt;/p&amp;gt;
&amp;lt;div class=&amp;#34;highlight&amp;#34;&amp;gt;&amp;lt;pre class=&amp;#34;chroma&amp;#34;&amp;gt;&amp;lt;code class=&amp;#34;language-latex&amp;#34; data-lang=&amp;#34;latex&amp;#34;&amp;gt;\mathrel{\raise{0.19ex}{\scriptstyle&#43;}} = J_f&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;p&amp;gt;which renders the as above.&amp;lt;/p&amp;gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Two tricks for change of variables in MCMC</title>
    <link href="http://tpapp.github.io/post/jacobian-chain/"/>
    <id>http://tpapp.github.io/post/jacobian-chain/</id>
    <published>2017-05-23T16:39:26+02:00</published>
    <updated>2017-05-23T16:39:26+02:00</updated>
    
    <content type="html" xml:base="http://tpapp.github.io/post/jacobian-chain/">

&amp;lt;p&amp;gt;Change of variables are sometimes advantageous, and occasionally inevitable for MCMC if you want efficient sampling, or to model a distribution that was obtained by a transformation. A classic example is the &amp;lt;em&amp;gt;lognormal distribution&amp;lt;/em&amp;gt;: when&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;$$\log(y) \sim N(\mu, \sigma^2)$$&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;one has to adjust the log posterior by $-\log y$ since&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;$$\frac{\partial \log(y)}{\partial y} = \frac{1}{y}$$&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;and&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;$$\log(1/y) = -\log(y).$$&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;In &amp;lt;a href=&amp;#34;http://mc-stan.org/&amp;#34;&amp;gt;Stan&amp;lt;/a&amp;gt;, one would accomplish this as&amp;lt;/p&amp;gt;
&amp;lt;div class=&amp;#34;highlight&amp;#34;&amp;gt;&amp;lt;pre class=&amp;#34;chroma&amp;#34;&amp;gt;&amp;lt;code class=&amp;#34;language-stan&amp;#34; data-lang=&amp;#34;stan&amp;#34;&amp;gt;target &#43;= -log(y)&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;p&amp;gt;In general, when you transform using a multivariate function $f$, you would adjust by&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;$$\log\det J_f(y)$$&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;which is the log of the determinant of the Jacobian &amp;amp;mdash; some texts simply refer to this as &amp;amp;ldquo;the Jacobian&amp;amp;rdquo;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;The above is well-known, but the following two tricks are worth mentioning.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;chaining-transformations&amp;#34;&amp;gt;Chaining transformations&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Suppose that you are changing a variable by using a chain of two functions $f \circ g$. Then
\begin{multline}
\log\det J_{f \circ g}(y) = \log \bigl(\det J_f(g(y)) \cdot \det J_g(y)\bigr) \\
= \log\det J_f(g(y)) &#43; \log\det J_g(y)
\end{multline}
which means that you can simply add (the log determinant of) the Jacobians, of course evaluated at the appropriate points.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;This is very useful when $f \circ g$ is complicated and $J_{f\circ g}$ is tedious to derive, or if you want to use multiple $f$s or $g$s and economize on the algebra.
 From the above, it is also easy to see that this generalizes to arbitrarily long chains of functions $f_1 \circ f_2 \circ \dots$.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;This trick turned out to be very useful when I was fitting a model where a transformation was general to both equilibrium concepts I was using (a noncooperative game and a social planner), so I could save on code. Of course, since &amp;lt;a href=&amp;#34;https://github.com/stan-dev/stan/issues/2224&amp;#34;&amp;gt;#2224&amp;lt;/a&amp;gt; is WIP, I had to copy-paste the code, but still saved quite a bit of work.&amp;lt;/p&amp;gt;

&amp;lt;h2 id=&amp;#34;transforming-a-subset-of-variables&amp;#34;&amp;gt;Transforming a subset of variables&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Suppose $x \in \mathbb{R}^m$ and $y \in \mathbb{R}^n$ are vectors, and you are interested in transforming to
$$
z = f(x,y)
$$
where $x$ and $z$ have the same dimension. It is useful to think about this transformation as
$$g(x,y) = [f(x,y), y]^\top$$
where $g : \mathbb{R}^{m&#43;n} \to \mathbb{R}^{m&#43;n}$. Since $y$ is mapped to itself,
$$
J_g = \begin{bmatrix}
J_{f,x} &amp;amp;amp; J_{f,y} \\
0 &amp;amp;amp; I
\end{bmatrix}
$$
has a block structure, where
$$
J_{f,x} = \frac{\partial f(x,y)}{\partial x}
$$
and similarly for $J_{f,y}$. For the calculation of the determinant, you can safely ignore the latter, and $\log \det I = 0$, so
$$
\log\det J_g = \log\det J_{f,x}
$$&amp;lt;/p&amp;gt;
</content>
  </entry>
  
</feed>
