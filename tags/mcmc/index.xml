<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MCMConTamás K. Papp&#39;s website</title>
  <link href="https://tpapp.github.io/index.xml" rel="self"/>
  <link href="https://tpapp.github.io/tags/mcmc/"/>
  <updated>2017-06-12T16:25:57+02:00</updated>
  
  <id>https://tpapp.github.io/tags/mcmc/</id>
  <author>
    <name>Tamás K. Papp</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  
  <entry>
    <title type="html">Sampling variation in effective sample size estimates (MCMC)</title>
    <link href="https://tpapp.github.io/post/ess-sampling/"/>
    <id>https://tpapp.github.io/post/ess-sampling/</id>
    <published>2017-06-12T16:25:57+02:00</published>
    <updated>2017-06-12T16:25:57+02:00</updated>
    
    <content type="html" xml:base="https://tpapp.github.io/post/ess-sampling/">&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;MCMC samples, used in Bayesian statistics, are not independent --- in fact, unless one uses specialized methods or &lt;a href=&#34;https://arxiv.org/abs/1701.02434&#34;&gt;modern HMC&lt;/a&gt;, posterior draws are usually at highly autocorrelated. For independent draws,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\text{variance of simulation mean} \propto \frac1N
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; is the sample size, but for correlated draws, one has to scale the sample size with a factor&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\tau = \frac{1}{1+2\sum_{k=1}^\infty \rho_k}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(\rho_k\)&lt;/span&gt; is the lag-&lt;span  class=&#34;math&#34;&gt;\(k\)&lt;/span&gt; autocorrelation. &lt;span  class=&#34;math&#34;&gt;\(\tau N\)&lt;/span&gt; is the &lt;em&gt;effective sample size&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Usually, &lt;span  class=&#34;math&#34;&gt;\(\rho_k\)&lt;/span&gt; is estimated from the data using the variogram&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
V_k = \frac{1}{N-k} \sum_{i=1}^{N-k} x_i x_{i+k}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;from which we obtain&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\rho_k = 1-\frac{V_k}{2\text{var}(x)}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where an estimator for the variance is also used. Then, to avoid using noisy estimates, we only add up to the last &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt; where&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\rho_{K} + \rho_{K+1} \ge 0
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I will call &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt; the &lt;em&gt;last lag&lt;/em&gt;. &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; does something slightly different, using FFT for autocorrelations, and cutting off at the first negative &lt;span  class=&#34;math&#34;&gt;\(\rho_K\)&lt;/span&gt;, but for HMC this does not make a whole lot of difference.&lt;/p&gt;

&lt;h2 id=&#34;the-sampling-variation&#34;&gt;The sampling variation&lt;/h2&gt;

&lt;p&gt;I was coding up the above calculation, and needed some unit tests. Surprisignly, I could not find anything on the sampling variation of &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, so I wrote some simulations in Julia (&lt;a href=&#34;../ess-sampling.jl&#34;&gt;source code for everything&lt;/a&gt;). I did the following simulation exercise:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;for a given autocorrelation coefficient &lt;span  class=&#34;math&#34;&gt;\(\phi\)&lt;/span&gt;, simulate &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; draws from the AR(1) process
&lt;span  class=&#34;math&#34;&gt;\(
x_t = \phi x_{t-1} + \sigma \epsilon_t
\qquad
\epsilon_t \sim \text{Normal}(0,1), \text{IID}
\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;calculate &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;repeat 1000 times and plot the results.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I use &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;, as these would be typical sample sizes, first for a fairly efficient algorithm, then for a more stubborn but still manageable posterior.&lt;/p&gt;

&lt;h2 id=&#34;iid-samples&#34;&gt;IID samples&lt;/h2&gt;

&lt;p&gt;Let &lt;span  class=&#34;math&#34;&gt;\(\phi=0\)&lt;/span&gt;, then we expect &lt;span  class=&#34;math&#34;&gt;\(\tau=1\)&lt;/span&gt; (red line in histogram, coefficient of variation on top).&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0\)&lt;/span&gt; (IID), &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi0N1000.svg&#34; alt=&#34;Results with $$\phi=0$$ (IID), $$N=1000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0\)&lt;/span&gt; (IID), &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi0N10000.svg&#34; alt=&#34;Results with $$\phi=0$$ (IID), $$N=10000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;With &lt;span  class=&#34;math&#34;&gt;\(1000\)&lt;/span&gt; samples, there is a lot of variation in ESS: 800 could show up very easily in practice. &lt;span  class=&#34;math&#34;&gt;\(600\)&lt;/span&gt; is not improbable either. Using up to &lt;span  class=&#34;math&#34;&gt;\(10\)&lt;/span&gt; lags is not uncommon. For &lt;span  class=&#34;math&#34;&gt;\(10000\)&lt;/span&gt; samples, the precision is improved considerably, we commonly use &lt;span  class=&#34;math&#34;&gt;\(2\)&lt;/span&gt; or &lt;span  class=&#34;math&#34;&gt;\(4\)&lt;/span&gt; lags. For both sample sizes, notice the high correlation between the last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, and &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;: given the method above, using more lags increases &lt;span  class=&#34;math&#34;&gt;\(\tau^{-1}\)&lt;/span&gt;, so this is to be expected.&lt;/p&gt;

&lt;h2 id=&#34;ar1-samples-with-rho05&#34;&gt;AR(1) samples with &lt;span  class=&#34;math&#34;&gt;\(\rho=0.5\)&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;This is a more autocorrelated process, here theory tells us that &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;=1/3.&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.5\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi05N1000.svg&#34; alt=&#34;Results with $$\phi=0.5$$, $$N=1000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.5\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi05N10000.svg&#34; alt=&#34;Results with $$\phi=0.5$$, $$N=10000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Notice that &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt; is now more dispersed, compared to the IID case. Even with 10000 samples, the coefficient of variation is 6%, with 1000 it is around 1/6. In practice, expect effective sample sizes all over the place.&lt;/p&gt;

&lt;h2 id=&#34;ar1-samples-with-rho08&#34;&gt;AR(1) samples with &lt;span  class=&#34;math&#34;&gt;\(\rho=0.8\)&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;This is an even more autocorrelated process, here theory tells us that &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;=1/9.&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.8\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=1000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi08N1000.svg&#34; alt=&#34;Results with $$\phi=0.8$$, $$N=1000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;Results with &lt;span  class=&#34;math&#34;&gt;\(\phi=0.8\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(N=10000\)&lt;/span&gt;. (a) &lt;span  class=&#34;math&#34;&gt;\(\tau\)&lt;/span&gt;, (b) last lag &lt;span  class=&#34;math&#34;&gt;\(K\)&lt;/span&gt;, (c) scatterplot.


&lt;img src=&#34;https://tpapp.github.io/post/ess-sampling/ess-phi08N10000.svg&#34; alt=&#34;Results with $$\phi=0.8$$, $$N=10000$$. (a) $$\tau$$, (b) last lag $$K$$, (c) scatterplot.&#34;&gt;
&lt;/p&gt;

&lt;p&gt;There is now so much variation that in order to get an estimate for ESS that we can use for comparing various MCMC implementations, we need to run much more than &lt;span  class=&#34;math&#34;&gt;\(1000\)&lt;/span&gt; samples.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For unit testing ESS calculations, I will need to use 10000 samples, with &lt;span  class=&#34;math&#34;&gt;\(\pm10\)&lt;/span&gt; or similar error bands.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As a rule of thumb, I would ignore less than 1.5x variation in ESS for 1000 samples, or run longer chains: it may be just random noise.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;bibliography&#34;&gt;Bibliography&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Gelman, Andrew, et al. 2013. Bayesian data analysis. 3rd edition. Chapman &amp;amp; Hall/CRC.&lt;/li&gt;
&lt;li&gt;Stan Development Team. 2016. Stan Modeling Language Users Guide and Reference Manual, Version 2.15.0. &lt;a href=&#34;http://mc-stan.org&#34;&gt;http://mc-stan.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Getting a nice &#43;= in LaTeX math</title>
    <link href="https://tpapp.github.io/post/latex-math-increment/"/>
    <id>https://tpapp.github.io/post/latex-math-increment/</id>
    <published>2017-05-24T12:59:53+02:00</published>
    <updated>2017-05-24T12:59:53+02:00</updated>
    
    <content type="html" xml:base="https://tpapp.github.io/post/latex-math-increment/">&lt;p&gt;I am working on an appendix for a paper that uses MCMC, and I decided to document some &lt;a href=&#34;&#34;&gt;change of varible calculations&lt;/a&gt; in the interest of reproducibility (they are quite complex, because of multivariate determinants). But how can I typeset them nicely in $\LaTeX$?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;\mathtt{target} += J_f&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;gives
$$
\mathtt{target} += J_f
$$
which is to be expected, as &lt;code&gt;+&lt;/code&gt; is a binary operator and &lt;code&gt;=&lt;/code&gt; is a relation, so $\LaTeX$ is not expecting them to show up this way.&lt;/p&gt;

&lt;p&gt;We can remedy this as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;\mathtt{target} \mathrel{+}= J_f&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;which shows up as
$$
\mathtt{target} \mathrel{+}= J_f
$$
which is an improvement, but is still not visually appealing.&lt;/p&gt;

&lt;p&gt;Making the &lt;code&gt;+&lt;/code&gt; a bit smaller with&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;\mathrel{\raisebox{0.19ex}{$\scriptstyle+$}}=}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;yields
$$
\mathtt{target} \mathrel{\raise{0.19ex}{\scriptstyle+}} = J_f
$$
which looks OK enough to preclude further tweaking. Note that &lt;a href=&#34;http://www.mathjax.org/&#34;&gt;MathJax&lt;/a&gt; does not support &lt;code&gt;\raisebox&lt;/code&gt;, but you can use&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;\mathrel{\raise{0.19ex}{\scriptstyle+}} = J_f&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;which renders the as above.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Two tricks for change of variables in MCMC</title>
    <link href="https://tpapp.github.io/post/jacobian-chain/"/>
    <id>https://tpapp.github.io/post/jacobian-chain/</id>
    <published>2017-05-23T16:39:26+02:00</published>
    <updated>2017-05-23T16:39:26+02:00</updated>
    
    <content type="html" xml:base="https://tpapp.github.io/post/jacobian-chain/">&lt;p&gt;Change of variables are sometimes advantageous, and occasionally inevitable for MCMC if you want efficient sampling, or to model a distribution that was obtained by a transformation. A classic example is the &lt;em&gt;lognormal distribution&lt;/em&gt;: when&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\log(y) \sim N(\mu, \sigma^2)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;one has to adjust the log posterior by &lt;span  class=&#34;math&#34;&gt;\(-\log y\)&lt;/span&gt; since&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\partial \log(y)}{\partial y} = \frac{1}{y}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\log(1/y) = -\log(y).\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;, one would accomplish this as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-stan&#34; data-lang=&#34;stan&#34;&gt;target += -log(y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;In general, when you transform using a multivariate function &lt;span  class=&#34;math&#34;&gt;\(f\)&lt;/span&gt;, you would adjust by&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\log\det J_f(y)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;which is the log of the determinant of the Jacobian — some texts
simply refer to this as &amp;quot;the Jacobian&amp;quot;.&lt;/p&gt;

&lt;p&gt;The above is well-known, but the following two tricks are worth mentioning.&lt;/p&gt;

&lt;h2 id=&#34;chaining-transformations&#34;&gt;Chaining transformations&lt;/h2&gt;

&lt;p&gt;Suppose that you are changing a variable by using a chain of two
functions &lt;span  class=&#34;math&#34;&gt;\(f \circ g\)&lt;/span&gt;. Then&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\log\det J_{f \circ g}(y) = \log \bigl(\det J_f(g(y)) \cdot \det J_g(y)\bigr) \\\\
= \log\det J_f(g(y)) + \log\det J_g(y)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;which means that you can simply add (the log determinant of) the
Jacobians, of course evaluated at the appropriate points.&lt;/p&gt;

&lt;p&gt;This is very useful when &lt;span  class=&#34;math&#34;&gt;\(f \circ g\)&lt;/span&gt; is complicated and &lt;span  class=&#34;math&#34;&gt;\(J_{f\circ g}\)&lt;/span&gt;
is tedious to derive, or if you want to use multiple &lt;span  class=&#34;math&#34;&gt;\(f\)&lt;/span&gt;s or &lt;span  class=&#34;math&#34;&gt;\(g\)&lt;/span&gt;s and
economize on the algebra.  From the above, it is also easy to see that this
generalizes to arbitrarily long chains of functions &lt;span  class=&#34;math&#34;&gt;\(f_1 \circ f_2 \circ \dots\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;This trick turned out to be very useful when I was fitting a model
where a transformation was general to both equilibrium concepts I was
using (a noncooperative game and a social planner), so I could save on
code. Of course, since
&lt;a href=&#34;https://github.com/stan-dev/stan/issues/2224&#34;&gt;#2224&lt;/a&gt; is WIP, I had to
copy-paste the code, but still saved quite a bit of work.&lt;/p&gt;

&lt;h2 id=&#34;transforming-a-subset-of-variables&#34;&gt;Transforming a subset of variables&lt;/h2&gt;

&lt;p&gt;Suppose &lt;span  class=&#34;math&#34;&gt;\(x \in \mathbb{R}^m\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(y \in \mathbb{R}^n\)&lt;/span&gt; are vectors, and you are interested in transforming to&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
z = f(x,y)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(z\)&lt;/span&gt; have the same dimension. It is useful to think
about this transformation as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
g(x,y) = [f(x,y), y]^\top
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(g : \mathbb{R}^{m+n} \to \mathbb{R}^{m+n}\)&lt;/span&gt;. Since &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; is mapped to itself,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
J_g = \begin{bmatrix}
J_{f,x} &amp; J_{f,y} \\\\
0 &amp; I
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;has a block structure, where&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
J_{f,x} = \frac{\partial f(x,y)}{\partial x}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;and similarly for &lt;span  class=&#34;math&#34;&gt;\(J_{f,y}\)&lt;/span&gt;. For the calculation of the determinant, you can safely ignore the latter, and &lt;span  class=&#34;math&#34;&gt;\(\log \det I = 0\)&lt;/span&gt;, so&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\log\det J_g = \log\det J_{f,x}
\]&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
  
</feed>
