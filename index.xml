<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tamás K. Papp&#39;s blog on Tamás K. Papp&#39;s blog</title>
    <link>http://tpapp.github.io/</link>
    <description>Recent content in Tamás K. Papp&#39;s blog on Tamás K. Papp&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2017, Tamás K. Papp. [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/legalcode).</copyright>
    <lastBuildDate>Wed, 24 May 2017 12:59:53 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Getting a nice &#43;= in LaTeX math</title>
      <link>http://tpapp.github.io/post/latex-math-increment/</link>
      <pubDate>Wed, 24 May 2017 12:59:53 +0200</pubDate>
      
      <guid>http://tpapp.github.io/post/latex-math-increment/</guid>
      <description>&lt;p&gt;I am working on an appendix for a paper that uses MCMC, and I decided to document some &lt;a href=&#34;http://tpapp.github.io/post/jacobian-chain/&#34;&gt;change of varible calculations&lt;/a&gt; in the interest of reproducibility (they are quite complex, because of multivariate determinants). But how can I typeset them nicely in $\LaTeX$?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;\mathtt{target} += J_f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;gives
$$
\mathtt{target} += J_f
$$
which is to be expected, as &lt;code&gt;+&lt;/code&gt; is a binary operator and &lt;code&gt;=&lt;/code&gt; is a relation, so $\LaTeX$ is not expecting them to show up this way.&lt;/p&gt;

&lt;p&gt;We can remedy this as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;\mathtt{target} \mathrel{+}= J_f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which shows up as
$$
\mathtt{target} \mathrel{+}= J_f
$$
which is an improvement, but is still not visually appealing.&lt;/p&gt;

&lt;p&gt;Making the &lt;code&gt;+&lt;/code&gt; a bit smaller with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;\mathrel{\raisebox{0.19ex}{$\scriptstyle+$}}=}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;yields
$$
\mathtt{target} \mathrel{\raise{0.19ex}{\scriptstyle+}} = J_f
$$
which looks OK enough to preclude further tweaking. Note that &lt;a href=&#34;http://www.mathjax.org/&#34;&gt;MathJax&lt;/a&gt; does not support &lt;code&gt;\raisebox&lt;/code&gt;, but you can use&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;\mathrel{\raise{0.19ex}{\scriptstyle+}} = J_f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which renders the as above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two tricks for change of variables in MCMC</title>
      <link>http://tpapp.github.io/post/jacobian-chain/</link>
      <pubDate>Tue, 23 May 2017 16:39:26 +0200</pubDate>
      
      <guid>http://tpapp.github.io/post/jacobian-chain/</guid>
      <description>

&lt;p&gt;Change of variables are sometimes advantageous, and occasionally inevitable for MCMC if you want efficient sampling, or to model a distribution that was obtained by a transformation. A classic example is the &lt;em&gt;lognormal distribution&lt;/em&gt;: when&lt;/p&gt;

&lt;p&gt;$$\log(y) \sim N(\mu, \sigma^2)$$&lt;/p&gt;

&lt;p&gt;one has to adjust the log posterior by $-\log y$ since&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \log(y)}{\partial y} = \frac{1}{y}$$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$$\log(1/y) = -\log(y).$$&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;, one would accomplish this as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stan&#34;&gt;target += -log(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In general, when you transform using a multivariate function $f$, you would adjust by&lt;/p&gt;

&lt;p&gt;$$\log\det J_f(y)$$&lt;/p&gt;

&lt;p&gt;which is the log of the determinant of the Jacobian &amp;mdash; some texts simply refer to this as &amp;ldquo;the Jacobian&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The above is well-known, but the following two tricks are worth mentioning.&lt;/p&gt;

&lt;h2 id=&#34;chaining-transformations&#34;&gt;Chaining transformations&lt;/h2&gt;

&lt;p&gt;Suppose that you are changing a variable by using a chain of two functions $f \circ g$. Then
\begin{multline}
\log\det J_{f \circ g}(y) = \log \bigl(\det J_f(g(y)) \cdot \det J_g(y)\bigr) \newline
= \log\det J_f(g(y)) + \log\det J_g(y)
\end{multline}
which means that you can simply add (the log determinant of) the Jacobians, of course evaluated at the appropriate points.&lt;/p&gt;

&lt;p&gt;This is very useful when $f \circ g$ is complicated and $J_{f\circ g}$ is tedious to derive, or if you want to use multiple $f$s or $g$s and economize on the algebra.
 From the above, it is also easy to see that this generalizes to arbitrarily long chains of functions $f_1 \circ f_2 \circ \dots$.&lt;/p&gt;

&lt;p&gt;This trick turned out to be very useful when I was fitting a model where a transformation was general to both equilibrium concepts I was using (a noncooperative game and a social planner), so I could save on code. Of course, since &lt;a href=&#34;https://github.com/stan-dev/stan/issues/2224&#34;&gt;#2224&lt;/a&gt; is WIP, I had to copy-paste the code, but still saved quite a bit of work.&lt;/p&gt;

&lt;h2 id=&#34;transforming-a-subset-of-variables&#34;&gt;Transforming a subset of variables&lt;/h2&gt;

&lt;p&gt;Suppose $x \in \mathbb{R}^m$ and $y \in \mathbb{R}^n$ are vectors, and you are interested in transforming to
$$
z = f(x,y)
$$
where $x$ and $z$ have the same dimension. It is useful to think about this transformation as
$$g(x,y) = [f(x,y), y]^\top$$
where $g : \mathbb{R}^{m+n} \to \mathbb{R}^{m+n}$. Since $y$ is mapped to itself,
$$
J_g = \begin{bmatrix}
J_{f,x} &amp;amp; J_{f,y} \newline
0 &amp;amp; I
\end{bmatrix}
$$
has a block structure, where
$$
J_{f,x} = \frac{\partial f(x,y)}{\partial x}
$$
and similarly for $J_{f,y}$. For the calculation of the determinant, you can safely ignore the latter, and $\log \det I = 0$, so
$$
\log\det J_g = \log\det J_{f,x}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disabling Privacy Badger</title>
      <link>http://tpapp.github.io/post/privacy-badger/</link>
      <pubDate>Tue, 02 May 2017 16:35:37 +0200</pubDate>
      
      <guid>http://tpapp.github.io/post/privacy-badger/</guid>
      <description>&lt;p&gt;Firefox has been my primary browser for the last decade. I find it very fast and convenient, and use quite a few addons, including &lt;a href=&#34;https://addons.mozilla.org/en-gb/firefox/addon/google-scholar-button/&#34;&gt;Google Scholar Button&lt;/a&gt;, &lt;a href=&#34;https://addons.mozilla.org/en-gb/firefox/addon/noscript/?src=search&#34;&gt;NoScript&lt;/a&gt;,  and &lt;a href=&#34;https://addons.mozilla.org/en-US/firefox/addon/ublock-origin/&#34;&gt;uBlock Origin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I can&amp;rsquo;t recall exactly when, but about half a year ago various sites started to break. I have not bothered to debug what&amp;rsquo;s happening, suspecting it was was a combination of various plugins, but continued to use Firefox and installed &lt;a href=&#34;https://github.com/darktrojan/openwith&#34;&gt;openwith&lt;/a&gt; to open broken pages in Chromium (yes, I know, the ultimate kludge). More sites became broken, and I found that now I am spending 99% of my time in Chromium, which I don&amp;rsquo;t like that much, but moreover, it is a resource hog: while using the plugins above I can get the CPU load of Firefox to around 1&amp;ndash;2% when I am not using it, Chromium drains my laptop battery in effectively half the time. To make things worse, Chromium apparently can&amp;rsquo;t open links in the background like Firefox, and insists on raising the window every time I open a link from another process, which is distracting.&lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&#34;https://discourse.julialang.org/&#34;&gt;Julia&amp;rsquo;s Discourse forum&lt;/a&gt; started showing up empty, which was the last straw and I went through my plugins. It turns out hat &lt;a href=&#34;https://www.eff.org/privacybadger&#34;&gt;Privacy Badger&lt;/a&gt; was responsible for everything: apparently it relies on heavy-handed heuristics and breaks a lot of webpages. One can report this, but instead of reporting around 10&amp;ndash;20 pages which were broken, I simply removed the plugin. I have a lot of respect for what the Electronic Frontier Foundation does, but I am not sure that this plugin is very useful.&lt;/p&gt;

&lt;p&gt;So finally I got back the web the way I like it. Moral of the story: temporary workarounds become permanent, and bite back.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Continuous-time deterministic dynamic programming in Julia</title>
      <link>http://tpapp.github.io/post/dynamic-programming/</link>
      <pubDate>Sun, 02 Apr 2017 19:06:04 +0200</pubDate>
      
      <guid>http://tpapp.github.io/post/dynamic-programming/</guid>
      <description>

&lt;p&gt;For the past few weeks I have been organizing pieces of code I have used to solve economic models into Julia packages. &lt;a href=&#34;https://github.com/tpapp/EconFunctions.jl&#34;&gt;EconFunctions.jl&lt;/a&gt; is a collection of trivial functions that I noticed that I kept recoding/copy-pasting everywhere, occasionally making errors. &lt;a href=&#34;https://github.com/tpapp/ContinuousTransformations.jl&#34;&gt;ContinuousTransformations.jl&lt;/a&gt; is a library for manipulating various commonly used homeomorphisms (univariate at the moment), which are useful in functional equations or Markov Chain Monte Carlo. Finally &lt;a href=&#34;https://github.com/tpapp/ParametricFunctions.jl&#34;&gt;ParametricFunctions.jl&lt;/a&gt; is for working with parametric function families.&lt;/p&gt;

&lt;p&gt;In this post I use these three to solve a simple, deterministic dynamic programming model in continuous time, known as the Ramsey growth model. If you are not an economist, it is very unlikely that it will make a lot of sense. If you are a student, I added basic references at the end, which are consistent with the methods in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Caveat&lt;/strong&gt;: &lt;em&gt;these libraries are in development, I am refining the API and changing things all the time. It is very likely that as time progresses, code in this post will not run without changes.&lt;/em&gt; In other words, treat this as a sneak peak into a library which is in development.&lt;/p&gt;

&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;

&lt;p&gt;This is standard material, I am just repeating it so that this post is self-contained. We solve&lt;/p&gt;

&lt;p&gt;$$\max \int_0^\infty e^{-\rho t} u(c_t) dt$$
subject to
$$\dot{k}_t = F(k_t) - c_t, k_t \ge 0 \forall t.$$&lt;/p&gt;

&lt;p&gt;where $u( c )$ is a CRRA utility function with IES $\theta$, $F(k) = A k^\alpha - \delta k$ is a production function that accounts for depreciation. Our problem is described by the Hamilton-Jacobi-Bellman equation&lt;/p&gt;

&lt;p&gt;$$\rho V(k) = \max_c u( c ) + (F(k)-c) V&amp;rsquo;(k)$$&lt;/p&gt;

&lt;p&gt;Notice that once we have $V$, the first-order condition&lt;/p&gt;

&lt;p&gt;$$u&amp;rsquo;(c(k)) = V&amp;rsquo;(k)$$&lt;/p&gt;

&lt;p&gt;yields the policy function $c(k)$, which we are interested in. Combining the envelope condition&lt;/p&gt;

&lt;p&gt;$$\rho V&amp;rsquo;(k) = F&amp;rsquo;(k) V&amp;rsquo;(k) + (F(k)-c) V{&amp;lsquo;}{&amp;rsquo;}(k)$$&lt;/p&gt;

&lt;p&gt;and using the functional form for CRRA utility, we obtain&lt;/p&gt;

&lt;p&gt;$$\frac{c&amp;rsquo;(k)}{c(k)} (F(k)-c(k)) = \frac{1}{\theta} (F&amp;rsquo;(k)-\rho)$$&lt;/p&gt;

&lt;p&gt;which is a recursive form of the so-called Euler equation.&lt;/p&gt;

&lt;p&gt;Also, note that we can characterize the steady state capital and consumption by&lt;/p&gt;

&lt;p&gt;$$k_s = \left(\frac{\delta+\rho}{A\alpha}\right)^{1/(\alpha-1)}$$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$$c_s = F(k_s)$$&lt;/p&gt;

&lt;h2 id=&#34;julia-code-solving-the-euler-equation&#34;&gt;Julia code: solving the Euler equation&lt;/h2&gt;

&lt;p&gt;Load the libraries (you need to clone some code, as the packages are not registered).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using ParametricFunctions       # unregistered, clone from repo
using ContinuousTransformations # unregistered, clone from repo
using EconFunctions             # unregistered, clone from repo
using Plots; gr()
using Parameters
using NLsolve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is useful to put model parameters in a single structure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot;&amp;quot;&amp;quot;
Very simple (normalized) Ramsey model with isoelastic production
function and utility.
&amp;quot;&amp;quot;&amp;quot;
@with_kw immutable RamseyModel{T}
    θ::T                        # IES
    α::T                        # capital share
    A::T                        # TFP
    ρ::T                        # discount rate
    δ::T                        # depreciation
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the key part: we code the residual for the Euler equation. The function should take the model (which contains the parameters), a function $c$ that has been constructed using a function family and a set of parameters, and a scalar $k$, at which we evaluate the residual above. Everything else can be automated very well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot;&amp;quot;&amp;quot;
Residual of the Euler equation.
&amp;quot;&amp;quot;&amp;quot;
function euler_residual(model::RamseyModel, c, k)
    @unpack θ, ρ, α, A, δ = model
    Fk = A*k^α - δ*k
    F′k = A*α*k^(α-1) - δ
    ck, c′k = c(ValuePartial(k))
    (c′k/ck)*(Fk-ck) - 1/θ*(F′k-ρ)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above, &lt;code&gt;c&lt;/code&gt; can be treated like an ordinary function, except that if you call it with &lt;code&gt;ValuePartial(x)&lt;/code&gt;, you get the value &lt;em&gt;and&lt;/em&gt; the derivative.&lt;/p&gt;

&lt;p&gt;The steady state will be handy:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot;Return the steady state capital and consumption for the model.&amp;quot;
function steady_state(model::RamseyModel)
    @unpack α, A, ρ, δ = model
    k = ((δ+ρ)/(A*α))^(1/(α-1))
    c = A*k^α - δ*k
    k, c
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s make a model object (parameters are pretty standard), and calculate the steady state:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;model = RamseyModel(θ = 2.0, α = 0.3, A = 1.0, ρ = 0.02, δ = 0.05)

kₛ, cₛ = steady_state(model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will solve in a domain around the steady state capital.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;kdom = (0.5*kₛ)..(2*kₛ)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given the pieces above, obtaining the solution can be done very conscisely: create a residual object, which is basically a mapping from parameters to the function family to the residuals:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;res = CollocationResidual(model, DomainTrans(kdom, Chebyshev(10)),
                          euler_residual)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above say that we want 10 Chebyshev polynomials, transformed to the domain &lt;code&gt;kdom&lt;/code&gt;, to be used for constructing the $c(k)$.&lt;/p&gt;

&lt;p&gt;We call the solver, providing an initial guess, $c(k) = k\cdot c_s/k_s$, for the policy function $c(k)$. The guess is that consumption is linear in capital, and the line goes through the steady state values. Other reasonable guesses are possible, but note that it is worthwhile thinking a bit about a good one, so that you get fast convergence.&lt;/p&gt;

&lt;p&gt;The function below fits a parametric function from the given family to the initial guess, then solves for the residual being $0$ using &lt;code&gt;NLsolve&lt;/code&gt; with automatic differentiation under the hood.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;c_sol, o = solve_collocation(res, k-&amp;gt;cₛ*k/kₛ; ftol=1e-10,
                             method = :newton)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Convergence statistics:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Results of Nonlinear Solver Algorithm
 * Algorithm: Newton with line-search
 * Starting Point: [1.83249,1.09949,7.10543e-16,4.44089e-16,-1.77636e-16,-8
.88178e-17,-8.43769e-16,1.64313e-15,1.33227e-16,3.10862e-16]
 * Zero: [1.57794,0.433992,-0.0360164,0.00624848,-0.00134301,0.000320829,-8
.21347e-5,2.28742e-5,-6.85183e-6,1.48105e-6]
 * Inf-norm of residuals: 0.000000
 * Iterations: 6
 * Convergence: true
   * |x - x&#39;| &amp;lt; 0.0e+00: false
   * |f(x)| &amp;lt; 1.0e-10: true
 * Function Calls (f): 7
 * Jacobian Calls (df/dx): 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Overall, pretty good, very few iterations. We plot the resulting function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(c_sol, xlab = &amp;quot;k&amp;quot;, ylab = &amp;quot;c(k)&amp;quot;, legend = false)
scatter!([kₛ], [cₛ])
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;../figures/dynamic-programming_10_1.svg&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Notive how the collocation nodes are added automatically (this is done with a plot recipe). It should, of course, go thought the steady state.&lt;/p&gt;

&lt;p&gt;It is very important to plot the residual:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(k-&amp;gt;euler_residual(model, c_sol, k), linspace(kdom, 100),
     legend = false, xlab=&amp;quot;k&amp;quot;, ylab=&amp;quot;Euler residual&amp;quot;)
scatter!(zero, points(c_sol))
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;../figures/dynamic-programming_11_1.svg&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Note the near-equioscillation property, which you get from using Chebyshev polynomials. You get $10^{-6}$ accuracy, which is neat (but note that this is a simple textbook problem, very smooth and tractable).&lt;/p&gt;

&lt;h2 id=&#34;selected-reading&#34;&gt;Selected reading&lt;/h2&gt;

&lt;p&gt;Acemoglu, Daron. Introduction to modern economic growth. Princeton University Press, 2008. &lt;em&gt;Chapter 8.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Miranda, Mario J., and Paul L. Fackler. Applied computational economics and finance. MIT press, 2004. &lt;em&gt;Chapters 10 and 11&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blogging with Hugo, Julia, Weave.jl</title>
      <link>http://tpapp.github.io/post/blogging-weave-julia-hugo/</link>
      <pubDate>Thu, 30 Mar 2017 10:02:32 +0200</pubDate>
      
      <guid>http://tpapp.github.io/post/blogging-weave-julia-hugo/</guid>
      <description>&lt;p&gt;I have made a PR to &lt;a href=&#34;https://github.com/mpastell/Weave.jl&#34;&gt;Weave.jl&lt;/a&gt; which Matti Pastell kindly merged recently. This allows a relatively smooth workflow for blogging using the static website generator &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;, and generating some pages with plots and evaluated Julia results. I made the source for my blog &lt;a href=&#34;https://github.com/tpapp/tpapp.github.io-source&#34;&gt;available&lt;/a&gt; so that others can use it for their own blogging about Julia. An example is &lt;a href=&#34;./post/hugo-julia-weave/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The gist of the workflow is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;for posts which do not need &lt;code&gt;Weave&lt;/code&gt;, just use &lt;code&gt;Hugo&lt;/code&gt;. Make sure you read their &lt;a href=&#34;https://gohugo.io/overview/introduction/&#34;&gt;excellent tutorial&lt;/a&gt;. &lt;strong&gt;This very fast&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;for posts which contain Julia code and generated plots, use a script to generate a skeleton file in a separate directory, and work on that. Call another script to generate the &lt;code&gt;.md&lt;/code&gt; file using &lt;code&gt;Weave.jl&lt;/code&gt;. &lt;strong&gt;This is the slow part&lt;/strong&gt;, so it is not automated.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/tpapp/tpapp.github.io-source&#34;&gt;README&lt;/a&gt; gives more details. Feel free to ask questions here. If you have a better workflow, I would like to hear about it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching a course using Julia</title>
      <link>http://tpapp.github.io/post/teaching-a-course/</link>
      <pubDate>Fri, 24 Mar 2017 14:34:22 +0100</pubDate>
      
      <guid>http://tpapp.github.io/post/teaching-a-course/</guid>
      <description>

&lt;p&gt;I just finished teaching a graduate course on practical aspects of dynamic optimization to our economics students. This year, for the first time, I taught the course using Julia, and this is a writeup of the experience. This was an intensive, 10-week course, with two classes per week, taught in the computer lab. A course on the theory of dynamic optimization was a prerequisite, this one was all about the actual numerical methods. The students had prior exposure to R and Matlab, and some of them have been using Julia for a while. In some classes, I talked about theory, sometimes I wrote code, ran it, and made improved versions, sometimes we treated the class as a practice session.&lt;/p&gt;

&lt;p&gt;I wanted to focus on the actual methods, so I decided to use a subset of the language, &amp;ldquo;Julia light&amp;rdquo;, using the following concepts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;scalars, algebra, arrays, indexing&lt;/li&gt;
&lt;li&gt;functions (with very basic dispatch, on an argument that contained problem parameters)&lt;/li&gt;
&lt;li&gt;control flow: &lt;code&gt;for&lt;/code&gt; and &lt;code&gt;if&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;comprehension, concatenation&lt;/li&gt;
&lt;li&gt;structures (only &lt;code&gt;immutable&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;docstrings&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The purpose of the course was to show that one can easily implement seemingly abstract methods encountered in textbooks, dissect them, look at the caveats, and possibly adapt them to particular problems. Writing what I think of as production code would have involved teaching many new concepts to a class coming with very heterogeneous experience in programming, so I decided to steer clear of the following topics:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;modules&lt;/li&gt;
&lt;li&gt;macros&lt;/li&gt;
&lt;li&gt;the type system&lt;/li&gt;
&lt;li&gt;writing efficient code (even though we ended up doing a bit on that, and benchmarking, could not resist)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We used &lt;a href=&#34;https://github.com/EconForge/NLsolve.jl&#34;&gt;NLsolve&lt;/a&gt;, &lt;a href=&#34;https://github.com/JuliaNLSolvers/Optim.jl&#34;&gt;Optim&lt;/a&gt;, and &lt;a href=&#34;https://github.com/JuliaPlots/Plots.jl&#34;&gt;Plots&lt;/a&gt; extensively, and &lt;a href=&#34;https://github.com/JuliaDiff/ForwardDiff.jl&#34;&gt;ForwardDiff&lt;/a&gt; under the hood. &lt;a href=&#34;https://github.com/mauro3/Parameters.jl&#34;&gt;Parameters&lt;/a&gt; was very useful for clean code.&lt;/p&gt;

&lt;h2 id=&#34;perspective-of-the-instructor&#34;&gt;Perspective of the instructor&lt;/h2&gt;

&lt;p&gt;Even when I wrote something in a suboptimal manner, it turned out to be fast enough. Julia is great in that respect. However, compilation time dominated almost everything that we did, especially for plots.&lt;/p&gt;

&lt;p&gt;I was using Jupyter notebooks, inspired by &lt;a href=&#34;https://math.mit.edu/classes/18.S096/iap17/&#34;&gt;18.S096&lt;/a&gt;. While I am much, much slower writing code in Jupyter compared to Emacs, I think that this turned out to be a benefit: jumping around between windows is very difficult to follow. &lt;a href=&#34;https://github.com/JuliaGizmos/Interact.jl&#34;&gt;Interact&lt;/a&gt; was just great.&lt;/p&gt;

&lt;p&gt;I made a small package for code we wrote in class, and distributed new code via &lt;code&gt;Pkg.update()&lt;/code&gt;. This worked well most of the time.&lt;/p&gt;

&lt;p&gt;We were using &lt;code&gt;v0.5.0&lt;/code&gt; and later transitioned to &lt;code&gt;v0.5.1&lt;/code&gt;, which was seamless.&lt;/p&gt;

&lt;p&gt;Since I was not using modules, sometimes the best way to extricate myself from a state was restarting the kernel. This became a running joke among the students (&amp;ldquo;when in doubt, restart the kernel&amp;rdquo;). This actually worked very well for the infamous &lt;a href=&#34;https://github.com/julialang/julia/issues/265&#34;&gt;#265&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jupyter is great for interactive notes in class. I mixed a great deal of marked up text and LaTeX equations into the workbooks. Problem sets were handed in using Jupyter notebooks, and the exam (solving a dynamic programming problem) was also written in and graded as a notebook.&lt;/p&gt;

&lt;p&gt;Unicode specials are addictive. Once you learn about &lt;code&gt;α&lt;/code&gt;, you never name a variable &lt;code&gt;alpha&lt;/code&gt; again.&lt;/p&gt;

&lt;h2 id=&#34;perspective-of-the-students&#34;&gt;Perspective of the students&lt;/h2&gt;

&lt;p&gt;I talked to the class at the end of the course about their experience with Julia, and some of them individually. The biggest issue for them was lack of easily searchable answers to common questions: for R and Matlab, a &amp;ldquo;how do you &amp;hellip;&amp;rdquo; query turns up 100+ answers because many people have encountered the problem before. This was not the case for Julia. Lack of examples was an especially difficult issue for plots.&lt;/p&gt;

&lt;p&gt;Debugging in Jupyter was difficult, since it mostly amounted to debugging by bisection, isolation, and occasionally printing. Students found some of the error messages cryptic (especially when it was about not having a matching method, since we did not really go into the type system).&lt;/p&gt;

&lt;p&gt;The most puzzling transient bugs were because of &lt;a href=&#34;https://github.com/julialang/julia/issues/265&#34;&gt;#265&lt;/a&gt; (&amp;ldquo;but I recompiled that function!&amp;rdquo;). This was solved by restarting the kernel, so the latter became somewhat of a panacea. Since compilation time dominated everything, this slowed things down considerably.&lt;/p&gt;

&lt;h2 id=&#34;takeaway&#34;&gt;Takeaway&lt;/h2&gt;

&lt;p&gt;Would definitely do it again. Even with the issues, Julia was the most comfortable language to teach in.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Julia &#43; Weave.jl &#43; hugo test</title>
      <link>http://tpapp.github.io/post/hugo-julia-weave/</link>
      <pubDate>Fri, 03 Mar 2017 13:11:53 +0100</pubDate>
      
      <guid>http://tpapp.github.io/post/hugo-julia-weave/</guid>
      <description>&lt;p&gt;Testing the &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; formatter for &lt;a href=&#34;https://github.com/mpastell/Weave.jl&#34;&gt;Weave.jl&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Testing inline code: &lt;code&gt;1+1=2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Testing math:
$$x^2+y^2 = \int_0^1 f(z) dz$$&lt;/p&gt;

&lt;p&gt;Testing code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;1+1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Testing proper highlighting:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function foo(x, y)
    x+y
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A plot:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;x = 1:10
y = x.^2
scatter(x, y, legend = false)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{{&amp;lt; figure src=&amp;ldquo;../figures/hugo-julia-weave_4_1.svg&amp;rdquo; title=&amp;ldquo;Caption for this plot&amp;rdquo;  &amp;gt; }}&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
